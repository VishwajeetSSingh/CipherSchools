Feature Engineering & Selection
Feature Engineering involves creating new features or modifying existing ones to improve the performance of machine learning models. It includes various methods such as handling missing values, encoding categorical variables, feature scaling, and feature creation.

Feature Engineering Methods
Handling Missing Values

Imputation: Fill missing values with mean, median, mode, or a specific value.
Example:

Feature 1	Feature 2	Feature 3
0.1	0.2	NaN
0.2	NaN	0.6
NaN	0.6	0.7
After imputation:

Feature 1	Feature 2	Feature 3
0.1	0.2	0.65
0.2	0.4	0.6
0.15	0.6	0.7
python
Copy code
import pandas as pd
from sklearn.impute import SimpleImputer

# Sample data
data = {
    'Feature1': [1.0, 2.0, None, 4.0, 5.0],
    'Feature2': [2.0, None, 4.0, 5.0, None],
    'Feature3': [None, 3.0, 3.5, 4.0, 4.5]
}
df = pd.DataFrame(data)

# Handling missing values
imputer = SimpleImputer(strategy='mean')
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)
print("After Imputation:\n", df_imputed)
Encoding Categorical Variables

One-Hot Encoding: Convert categorical variables into a series of binary columns.
Example:

Color
Red
Blue
Green
Blue
Red
After one-hot encoding:

Color_Red	Color_Blue	Color_Green
1	0	0
0	1	0
0	0	1
0	1	0
1	0	0
python
Copy code
from sklearn.preprocessing import OneHotEncoder

# Sample data
data = {
    'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']
}
df = pd.DataFrame(data)

# Encoding categorical variables
encoder = OneHotEncoder(sparse=False)
encoded_categories = encoder.fit_transform(df[['Color']])
df_encoded = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(['Color']))
df = pd.concat([df, df_encoded], axis=1).drop('Color', axis=1)
print("After One-Hot Encoding:\n", df)
Feature Scaling

Min-Max Scaling: Scale features to a fixed range, typically [0, 1].
Example:

Feature 1	Feature 2
10	100
20	200
30	300
40	400
50	500
After min-max scaling:

Feature 1	Feature 2
0	0
0.5	0.5
1	1
python
Copy code
from sklearn.preprocessing import MinMaxScaler

# Sample data
data = {
    'Feature1': [10, 20, 30, 40, 50],
    'Feature2': [100, 200, 300, 400, 500]
}
df = pd.DataFrame(data)

# Feature scaling
scaler = MinMaxScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)
print("After Min-Max Scaling:\n", df_scaled)
Feature Creation

Polynomial Features: Create new features by taking polynomial combinations of existing features.
Example:

Feature 1	Feature 2
1	2
3	4
5	6
After creating polynomial features (degree=2):

Feature 1	Feature 2	Feature 1^2	Feature 2^2	Feature 1*Feature 2
1	2	1	4	2
3	4	9	16	12
5	6	25	36	30
python
Copy code
from sklearn.preprocessing import PolynomialFeatures

# Sample data
data = {
    'Feature1': [1, 2, 3, 4, 5],
    'Feature2': [2, 3, 4, 5, 6]
}
df = pd.DataFrame(data)

# Feature creation
poly = PolynomialFeatures(degree=2, include_bias=False)
poly_features = poly.fit_transform(df)
df_poly = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(['Feature1', 'Feature2']))
print("After Creating Polynomial Features:\n", df_poly)
Feature Selection Methods
Variance Thresholding

Explanation: Removes features with variance below a certain threshold. By default, it removes all zero-variance features.
Example:

Feature 1	Feature 2	Feature 3	Constant
1	2	3	1
1	3	4	1
1	4	5	1
1	5	6	1
1	6	7	1
In this example, Feature 1 and Constant have low or zero variance.

python
Copy code
from sklearn.feature_selection import VarianceThreshold

# Sample data
data = {
    'Feature1': [1, 1, 1, 1, 1],
    'Feature2': [2, 3, 4, 5, 6],
    'Feature3': [3, 4, 5, 6, 7],
    'Constant': [1, 1, 1, 1, 1]
}
df = pd.DataFrame(data)

# Variance Thresholding
selector = VarianceThreshold(threshold=0.1)
df_selected = pd.DataFrame(selector.fit_transform(df), columns=df.columns[selector.get_support()])
print("After Variance Thresholding:\n", df_selected)
Correlation Matrix Filtering

Explanation: Compute the correlation matrix and remove one of each pair of highly correlated features to reduce redundancy.
Example:

Feature 1	Feature 2	Feature 3	Feature 4
1	2	2	5
2	4	4	6
3	6	6	7
4	8	8	8
5	10	10	9
In this example, Feature 2 and Feature 3 are highly correlated with Feature 1.

python
Copy code
import seaborn as sns

# Sample data
df = pd.DataFrame({
    'Feature1': [1, 2, 3, 4, 5],
    'Feature2': [2, 4, 6, 8, 10],
    'Feature3': [2, 4, 6, 8, 10],
    'Feature4': [5, 6, 7, 8, 9]
})

# Compute correlation matrix
corr_matrix = df.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.show()
Domain Knowledge

Explanation: Utilize expertise from the specific field to manually select the most relevant features. This method leverages human understanding of which features are likely to be important.
Example:

| Age | Salary | Height | Weight |
|-----|--------|--------
